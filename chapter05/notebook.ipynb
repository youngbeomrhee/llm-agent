{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6x7b3Rpfgpk0"
   },
   "source": [
    "# 5. LangChain Expression Language(LCEL) 심층 해설\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dotenv in /Users/youngbeom-rhee/Repositories/study/llm-agent/.venv/lib/python3.12/site-packages (0.9.9)\n",
      "Requirement already satisfied: python-dotenv in /Users/youngbeom-rhee/Repositories/study/llm-agent/.venv/lib/python3.12/site-packages (from dotenv) (1.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:32:34.489407Z",
     "iopub.status.busy": "2024-06-28T02:32:34.488775Z",
     "iopub.status.idle": "2024-06-28T02:32:34.491583Z",
     "shell.execute_reply": "2024-06-28T02:32:34.491086Z"
    },
    "id": "jvuKOwNbgpk1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# from google.colab import userdata\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"] = userdata.get(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"agent-book\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "CQ4fwamMgpk1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-core==0.3.0\n",
      "  Downloading langchain_core-0.3.0-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting langchain-openai==0.2.0\n",
      "  Downloading langchain_openai-0.2.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting langchain-community==0.3.0\n",
      "  Downloading langchain_community-0.3.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting pydantic==2.10.6\n",
      "  Downloading pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting PyYAML>=5.3 (from langchain-core==0.3.0)\n",
      "  Downloading pyyaml-6.0.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (2.4 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core==0.3.0)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.117 (from langchain-core==0.3.0)\n",
      "  Downloading langsmith-0.1.147-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting packaging<25,>=23.2 (from langchain-core==0.3.0)\n",
      "  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain-core==0.3.0)\n",
      "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting typing-extensions>=4.7 (from langchain-core==0.3.0)\n",
      "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic==2.10.6)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic==2.10.6)\n",
      "  Downloading pydantic_core-2.27.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting openai<2.0.0,>=1.40.0 (from langchain-openai==0.2.0)\n",
      "  Downloading openai-1.109.1-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting tiktoken<1,>=0.7 (from langchain-openai==0.2.0)\n",
      "  Downloading tiktoken-0.12.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain-community==0.3.0)\n",
      "  Downloading sqlalchemy-2.0.43-cp312-cp312-macosx_11_0_arm64.whl.metadata (9.6 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain-community==0.3.0)\n",
      "  Downloading aiohttp-3.13.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (8.1 kB)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community==0.3.0)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting langchain<0.4.0,>=0.3.0 (from langchain-community==0.3.0)\n",
      "  Downloading langchain-0.3.27-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting numpy<2.0.0,>=1.26.0 (from langchain-community==0.3.0)\n",
      "  Downloading numpy-1.26.4-cp312-cp312-macosx_11_0_arm64.whl.metadata (61 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community==0.3.0)\n",
      "  Downloading pydantic_settings-2.11.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting requests<3,>=2 (from langchain-community==0.3.0)\n",
      "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.0)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.0)\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.0)\n",
      "  Downloading attrs-25.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.0)\n",
      "  Downloading frozenlist-1.8.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (20 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.0)\n",
      "  Downloading multidict-6.7.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.0)\n",
      "  Downloading propcache-0.4.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (13 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.0)\n",
      "  Downloading yarl-1.22.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (75 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.0)\n",
      "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.0)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core==0.3.0)\n",
      "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "INFO: pip is looking at multiple versions of langchain to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting langchain<0.4.0,>=0.3.0 (from langchain-community==0.3.0)\n",
      "  Downloading langchain-0.3.26-py3-none-any.whl.metadata (7.8 kB)\n",
      "  Downloading langchain-0.3.25-py3-none-any.whl.metadata (7.8 kB)\n",
      "  Downloading langchain-0.3.24-py3-none-any.whl.metadata (7.8 kB)\n",
      "  Downloading langchain-0.3.23-py3-none-any.whl.metadata (7.8 kB)\n",
      "  Downloading langchain-0.3.22-py3-none-any.whl.metadata (7.8 kB)\n",
      "  Downloading langchain-0.3.21-py3-none-any.whl.metadata (7.8 kB)\n",
      "  Downloading langchain-0.3.20-py3-none-any.whl.metadata (7.7 kB)\n",
      "INFO: pip is still looking at multiple versions of langchain to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading langchain-0.3.19-py3-none-any.whl.metadata (7.9 kB)\n",
      "  Downloading langchain-0.3.18-py3-none-any.whl.metadata (7.8 kB)\n",
      "  Downloading langchain-0.3.17-py3-none-any.whl.metadata (7.1 kB)\n",
      "  Downloading langchain-0.3.16-py3-none-any.whl.metadata (7.1 kB)\n",
      "  Downloading langchain-0.3.15-py3-none-any.whl.metadata (7.1 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading langchain-0.3.14-py3-none-any.whl.metadata (7.1 kB)\n",
      "  Downloading langchain-0.3.13-py3-none-any.whl.metadata (7.1 kB)\n",
      "  Downloading langchain-0.3.12-py3-none-any.whl.metadata (7.1 kB)\n",
      "  Downloading langchain-0.3.11-py3-none-any.whl.metadata (7.1 kB)\n",
      "  Downloading langchain-0.3.10-py3-none-any.whl.metadata (7.1 kB)\n",
      "  Downloading langchain-0.3.9-py3-none-any.whl.metadata (7.1 kB)\n",
      "  Downloading langchain-0.3.8-py3-none-any.whl.metadata (7.1 kB)\n",
      "  Downloading langchain-0.3.7-py3-none-any.whl.metadata (7.1 kB)\n",
      "  Downloading langchain-0.3.6-py3-none-any.whl.metadata (7.1 kB)\n",
      "  Downloading langchain-0.3.5-py3-none-any.whl.metadata (7.1 kB)\n",
      "  Downloading langchain-0.3.4-py3-none-any.whl.metadata (7.1 kB)\n",
      "  Downloading langchain-0.3.3-py3-none-any.whl.metadata (7.1 kB)\n",
      "  Downloading langchain-0.3.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "  Downloading langchain-0.3.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "  Downloading langchain-0.3.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain<0.4.0,>=0.3.0->langchain-community==0.3.0)\n",
      "  Downloading langchain_text_splitters-0.3.11-py3-none-any.whl.metadata (1.8 kB)\n",
      "INFO: pip is looking at multiple versions of langchain-text-splitters to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading langchain_text_splitters-0.3.10-py3-none-any.whl.metadata (1.9 kB)\n",
      "  Downloading langchain_text_splitters-0.3.9-py3-none-any.whl.metadata (1.9 kB)\n",
      "  Downloading langchain_text_splitters-0.3.8-py3-none-any.whl.metadata (1.9 kB)\n",
      "  Downloading langchain_text_splitters-0.3.7-py3-none-any.whl.metadata (1.9 kB)\n",
      "  Downloading langchain_text_splitters-0.3.6-py3-none-any.whl.metadata (1.9 kB)\n",
      "  Downloading langchain_text_splitters-0.3.5-py3-none-any.whl.metadata (2.3 kB)\n",
      "  Downloading langchain_text_splitters-0.3.4-py3-none-any.whl.metadata (2.3 kB)\n",
      "INFO: pip is still looking at multiple versions of langchain-text-splitters to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading langchain_text_splitters-0.3.3-py3-none-any.whl.metadata (2.3 kB)\n",
      "  Downloading langchain_text_splitters-0.3.2-py3-none-any.whl.metadata (2.3 kB)\n",
      "  Downloading langchain_text_splitters-0.3.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "  Downloading langchain_text_splitters-0.3.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from langsmith<0.2.0,>=0.1.117->langchain-core==0.3.0)\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.117->langchain-core==0.3.0)\n",
      "  Downloading orjson-3.11.3-cp312-cp312-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl.metadata (41 kB)\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.2.0,>=0.1.117->langchain-core==0.3.0)\n",
      "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting anyio (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.117->langchain-core==0.3.0)\n",
      "  Downloading anyio-4.11.0-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting certifi (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.117->langchain-core==0.3.0)\n",
      "  Downloading certifi-2025.10.5-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.117->langchain-core==0.3.0)\n",
      "  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting idna (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.117->langchain-core==0.3.0)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.117->langchain-core==0.3.0)\n",
      "  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai<2.0.0,>=1.40.0->langchain-openai==0.2.0)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai<2.0.0,>=1.40.0->langchain-openai==0.2.0)\n",
      "  Downloading jiter-0.11.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Collecting sniffio (from openai<2.0.0,>=1.40.0->langchain-openai==0.2.0)\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting tqdm>4 (from openai<2.0.0,>=1.40.0->langchain-openai==0.2.0)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /Users/youngbeom-rhee/Repositories/study/llm-agent/.venv/lib/python3.12/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community==0.3.0) (1.1.1)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community==0.3.0)\n",
      "  Downloading typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests<3,>=2->langchain-community==0.3.0)\n",
      "  Downloading charset_normalizer-3.4.3-cp312-cp312-macosx_10_13_universal2.whl.metadata (36 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2->langchain-community==0.3.0)\n",
      "  Downloading urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting regex>=2022.1.18 (from tiktoken<1,>=0.7->langchain-openai==0.2.0)\n",
      "  Downloading regex-2025.9.18-cp312-cp312-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.0)\n",
      "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Downloading langchain_core-0.3.0-py3-none-any.whl (405 kB)\n",
      "Downloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "Downloading langchain_openai-0.2.0-py3-none-any.whl (51 kB)\n",
      "Downloading langchain_community-0.3.0-py3-none-any.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.27.2-cp312-cp312-macosx_11_0_arm64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.13.0-cp312-cp312-macosx_11_0_arm64.whl (489 kB)\n",
      "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading langchain-0.3.0-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_text_splitters-0.3.0-py3-none-any.whl (25 kB)\n",
      "Downloading langsmith-0.1.147-py3-none-any.whl (311 kB)\n",
      "Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Downloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading multidict-6.7.0-cp312-cp312-macosx_11_0_arm64.whl (43 kB)\n",
      "Downloading numpy-1.26.4-cp312-cp312-macosx_11_0_arm64.whl (13.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading openai-1.109.1-py3-none-any.whl (948 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m948.6/948.6 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading anyio-4.11.0-py3-none-any.whl (109 kB)\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading jiter-0.11.0-cp312-cp312-macosx_11_0_arm64.whl (316 kB)\n",
      "Downloading orjson-3.11.3-cp312-cp312-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl (238 kB)\n",
      "Downloading packaging-24.2-py3-none-any.whl (65 kB)\n",
      "Downloading pydantic_settings-2.11.0-py3-none-any.whl (48 kB)\n",
      "Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Downloading charset_normalizer-3.4.3-cp312-cp312-macosx_10_13_universal2.whl (205 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Downloading sqlalchemy-2.0.43-cp312-cp312-macosx_11_0_arm64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
      "Downloading tiktoken-0.12.0-cp312-cp312-macosx_11_0_arm64.whl (994 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m994.0/994.0 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Downloading yarl-1.22.0-cp312-cp312-macosx_11_0_arm64.whl (94 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading attrs-25.4.0-py3-none-any.whl (67 kB)\n",
      "Downloading certifi-2025.10.5-py3-none-any.whl (163 kB)\n",
      "Downloading frozenlist-1.8.0-cp312-cp312-macosx_11_0_arm64.whl (50 kB)\n",
      "Downloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
      "Downloading propcache-0.4.1-cp312-cp312-macosx_11_0_arm64.whl (47 kB)\n",
      "Downloading pyyaml-6.0.3-cp312-cp312-macosx_11_0_arm64.whl (173 kB)\n",
      "Downloading regex-2025.9.18-cp312-cp312-macosx_11_0_arm64.whl (287 kB)\n",
      "Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: urllib3, typing-extensions, tqdm, tenacity, sniffio, regex, PyYAML, propcache, packaging, orjson, numpy, mypy-extensions, multidict, jsonpointer, jiter, idna, h11, frozenlist, distro, charset_normalizer, certifi, attrs, annotated-types, aiohappyeyeballs, yarl, typing-inspection, typing-inspect, SQLAlchemy, requests, pydantic-core, marshmallow, jsonpatch, httpcore, anyio, aiosignal, tiktoken, requests-toolbelt, pydantic, httpx, dataclasses-json, aiohttp, pydantic-settings, openai, langsmith, langchain-core, langchain-text-splitters, langchain-openai, langchain, langchain-community\n",
      "\u001b[2K  Attempting uninstall: packaging━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/49\u001b[0m [regex]\n",
      "\u001b[2K    Found existing installation: packaging 25.0━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/49\u001b[0m [regex]\n",
      "\u001b[2K    Uninstalling packaging-25.0:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/49\u001b[0m [regex]\n",
      "\u001b[2K      Successfully uninstalled packaging-25.0━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/49\u001b[0m [regex]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49/49\u001b[0m [langchain-community]ommunity]penai]litters]\n",
      "\u001b[1A\u001b[2KSuccessfully installed PyYAML-6.0.3 SQLAlchemy-2.0.43 aiohappyeyeballs-2.6.1 aiohttp-3.13.0 aiosignal-1.4.0 annotated-types-0.7.0 anyio-4.11.0 attrs-25.4.0 certifi-2025.10.5 charset_normalizer-3.4.3 dataclasses-json-0.6.7 distro-1.9.0 frozenlist-1.8.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 idna-3.10 jiter-0.11.0 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.0 langchain-community-0.3.0 langchain-core-0.3.0 langchain-openai-0.2.0 langchain-text-splitters-0.3.0 langsmith-0.1.147 marshmallow-3.26.1 multidict-6.7.0 mypy-extensions-1.1.0 numpy-1.26.4 openai-1.109.1 orjson-3.11.3 packaging-24.2 propcache-0.4.1 pydantic-2.10.6 pydantic-core-2.27.2 pydantic-settings-2.11.0 regex-2025.9.18 requests-2.32.5 requests-toolbelt-1.0.0 sniffio-1.3.1 tenacity-8.5.0 tiktoken-0.12.0 tqdm-4.67.1 typing-extensions-4.15.0 typing-inspect-0.9.0 typing-inspection-0.4.2 urllib3-2.5.0 yarl-1.22.0\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain-core==0.3.0 langchain-openai==0.2.0 langchain-community==0.3.0 pydantic==2.10.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QkvQCCUQgpk1"
   },
   "source": [
    "## 5.1. Runnable과 RunnableSequence―LCEL의 가장 기본적인 구성 요소  \n",
    "\n",
    "LECL을 잘 이해할 수 있도록 먼저 기존에 사용된 Runnable을 상속한 클래스를 모두 invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:33:12.290335Z",
     "iopub.status.busy": "2024-06-28T02:33:12.290156Z",
     "iopub.status.idle": "2024-06-28T02:33:12.344661Z",
     "shell.execute_reply": "2024-06-28T02:33:12.344241Z"
    },
    "id": "iw47kajfgpk2"
   },
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"사용자가 입력한 요리의 레시피를 생각해 주세요.\"),\n",
    "        (\"human\", \"{dish}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:33:12.346689Z",
     "iopub.status.busy": "2024-06-28T02:33:12.346510Z",
     "iopub.status.idle": "2024-06-28T02:33:21.108437Z",
     "shell.execute_reply": "2024-06-28T02:33:21.108007Z"
    },
    "id": "v1e4ytz_gpk2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "카레는 다양한 재료와 향신료를 사용하여 만드는 맛있는 요리입니다. 아래는 기본적인 카레 레시피입니다.\n",
      "\n",
      "### 재료\n",
      "- 고기 (닭고기, 소고기, 양고기 등) 300g\n",
      "- 양파 1개\n",
      "- 감자 1개\n",
      "- 당근 1개\n",
      "- 카레 가루 2-3 큰술\n",
      "- 식용유 2 큰술\n",
      "- 물 3컵\n",
      "- 소금, 후추 약간\n",
      "- 선택 재료: 피망, 버섯, 완두콩 등\n",
      "\n",
      "### 조리 방법\n",
      "1. **재료 손질**: 고기는 한 입 크기로 자르고, 양파는 다지고, 감자와 당근은 깍둑썰기 합니다.\n",
      "\n",
      "2. **양파 볶기**: 큰 냄비에 식용유를 두르고 중불에서 다진 양파를 넣어 투명해질 때까지 볶습니다.\n",
      "\n",
      "3. **고기 추가**: 양파가 볶아지면 고기를 넣고 겉면이 익을 때까지 볶습니다.\n",
      "\n",
      "4. **채소 추가**: 감자와 당근을 넣고 함께 볶아줍니다.\n",
      "\n",
      "5. **물 붓기**: 재료가 잘 섞이면 물을 붓고 끓입니다. 끓기 시작하면 불을 줄이고 중약불로 15-20분 정도 끓입니다.\n",
      "\n",
      "6. **카레 가루 추가**: 카레 가루를 넣고 잘 섞은 후, 다시 10분 정도 끓입니다. 필요에 따라 소금과 후추로 간을 맞춥니다.\n",
      "\n",
      "7. **완성**: 카레가 걸쭉해지면 불을 끄고, 밥과 함께 서빙합니다.\n",
      "\n",
      "### 팁\n",
      "- 카레는 시간이 지날수록 맛이 깊어지므로, 하루 정도 숙성시키면 더욱 맛있습니다.\n",
      "- 다양한 채소나 해산물을 추가하여 나만의 카레를 만들어 보세요.\n",
      "\n",
      "맛있게 드세요!\n"
     ]
    }
   ],
   "source": [
    "# 순서대로 invoke\n",
    "prompt_value = prompt.invoke({\"dish\": \"카레\"})\n",
    "ai_message = model.invoke(prompt_value)\n",
    "output = output_parser.invoke(ai_message)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:33:21.110332Z",
     "iopub.status.busy": "2024-06-28T02:33:21.110173Z",
     "iopub.status.idle": "2024-06-28T02:33:21.112634Z",
     "shell.execute_reply": "2024-06-28T02:33:21.112238Z"
    },
    "id": "Y1_uIxQ4gpk2"
   },
   "outputs": [],
   "source": [
    "# LCEL에서는 Runnable을 | 연산자로 연결할 수 있음\n",
    "chain = prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:33:21.114678Z",
     "iopub.status.busy": "2024-06-28T02:33:21.114418Z",
     "iopub.status.idle": "2024-06-28T02:33:26.539851Z",
     "shell.execute_reply": "2024-06-28T02:33:26.539250Z"
    },
    "id": "sidnFryBgpk2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "카레는 다양한 재료와 향신료를 사용하여 만드는 맛있는 요리입니다. 아래는 기본적인 카레 레시피입니다.\n",
      "\n",
      "### 재료\n",
      "- 고기 (닭고기, 소고기, 양고기 등) 300g\n",
      "- 양파 1개\n",
      "- 감자 1개\n",
      "- 당근 1개\n",
      "- 카레 가루 2-3 큰술\n",
      "- 식용유 2 큰술\n",
      "- 물 3컵\n",
      "- 소금, 후추 약간\n",
      "- 선택 재료: 마늘, 생강, 피망, 버섯 등\n",
      "\n",
      "### 조리 방법\n",
      "1. **재료 손질**: 고기는 한 입 크기로 자르고, 양파는 다지고, 감자와 당근은 깍둑썰기 합니다. 선택 재료도 적당한 크기로 썰어줍니다.\n",
      "\n",
      "2. **양파 볶기**: 큰 냄비에 식용유를 두르고 중불에서 다진 양파를 넣고 투명해질 때까지 볶습니다.\n",
      "\n",
      "3. **고기 추가**: 양파가 볶아지면 고기를 넣고 겉면이 익을 때까지 볶습니다.\n",
      "\n",
      "4. **채소 추가**: 감자와 당근을 넣고 함께 볶아줍니다. 이때 마늘과 생강을 추가하면 향이 더 좋아집니다.\n",
      "\n",
      "5. **물 붓기**: 모든 재료가 잘 섞이면 물을 붓고 끓입니다. 끓기 시작하면 불을 줄이고 중약불에서 15-20분 정도 끓입니다.\n",
      "\n",
      "6. **카레 가루 추가**: 카레 가루를 넣고 잘 섞은 후, 다시 10분 정도 끓입니다. 필요에 따라 소금과 후추로 간을 맞춥니다.\n",
      "\n",
      "7. **완성**: 카레가 걸쭉해지면 불을 끄고, 밥과 함께 서빙합니다.\n",
      "\n",
      "### 팁\n",
      "- 카레는 시간이 지날수록 맛이 깊어지므로, 하루 정도 숙성시키면 더욱 맛있습니다.\n",
      "- 다양한 재료를 추가하여 나만의 카레를 만들어 보세요!\n",
      "\n",
      "맛있게 드세요!\n"
     ]
    }
   ],
   "source": [
    "output = chain.invoke({\"dish\": \"카레\"})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CyCfUHUngpk2"
   },
   "source": [
    "### Runnable의 실행 방법―invoke・stream・batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:33:26.545129Z",
     "iopub.status.busy": "2024-06-28T02:33:26.544905Z",
     "iopub.status.idle": "2024-06-28T02:33:32.478679Z",
     "shell.execute_reply": "2024-06-28T02:33:32.478134Z"
    },
    "id": "G44R6pakgpk3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "카레는 다양한 재료와 향신료를 사용하여 만드는 맛있는 요리입니다. 아래는 기본적인 카레 레시피입니다.\n",
      "\n",
      "### 재료\n",
      "- 고기 (닭고기, 소고기, 양고기 등) 300g\n",
      "- 양파 1개\n",
      "- 감자 1개\n",
      "- 당근 1개\n",
      "- 카레 가루 2-3 큰술\n",
      "- 식용유 2 큰술\n",
      "- 물 3컵\n",
      "- 소금, 후추 약간\n",
      "- 선택 재료: 마늘, 생강, 피망, 버섯 등\n",
      "\n",
      "### 조리 방법\n",
      "1. **재료 손질**: 고기는 한 입 크기로 자르고, 양파는 다지고, 감자와 당근은 깍둑썰기 합니다. 선택 재료가 있다면 함께 손질합니다.\n",
      "\n",
      "2. **양파 볶기**: 큰 냄비에 식용유를 두르고 중불에서 다진 양파를 넣고 볶아줍니다. 양파가 투명해질 때까지 볶습니다.\n",
      "\n",
      "3. **고기 추가**: 손질한 고기를 넣고 겉면이 익을 때까지 볶습니다. 이때 마늘과 생강을 추가하면 향이 더 좋습니다.\n",
      "\n",
      "4. **채소 추가**: 감자와 당근을 넣고 함께 볶아줍니다. 다른 채소도 이 단계에서 추가할 수 있습니다.\n",
      "\n",
      "5. **물 붓기**: 모든 재료가 잘 섞이면 물을 붓고 끓입니다. 끓기 시작하면 불을 줄이고 중약불에서 15-20분 정도 끓입니다.\n",
      "\n",
      "6. **카레 가루 추가**: 카레 가루를 넣고 잘 섞어줍니다. 원하는 농도에 따라 물을 추가할 수 있습니다. 소금과 후추로 간을 맞춥니다.\n",
      "\n",
      "7. **완성**: 카레가 걸쭉해지면 불을 끄고, 5분 정도 뜸을 들인 후 그릇에 담아냅니다.\n",
      "\n",
      "### 서빙\n",
      "밥과 함께 서빙하면 좋습니다. 또한, 피클이나 요거트와 함께 곁들여 먹으면 더욱 맛있습니다.\n",
      "\n",
      "맛있게 드세요!"
     ]
    }
   ],
   "source": [
    "chain = prompt | model | output_parser\n",
    "\n",
    "for chunk in chain.stream({\"dish\": \"카레\"}):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:33:32.480592Z",
     "iopub.status.busy": "2024-06-28T02:33:32.480406Z",
     "iopub.status.idle": "2024-06-28T02:33:38.976064Z",
     "shell.execute_reply": "2024-06-28T02:33:38.974376Z"
    },
    "id": "2pol6DGIgpk3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['카레는 다양한 재료와 향신료를 사용하여 만드는 맛있는 요리입니다. 아래는 기본적인 카레 레시피입니다.\\n\\n### 재료\\n- 고기 (닭고기, 소고기, 돼지고기 등) 300g\\n- 양파 1개\\n- 감자 1개\\n- 당근 1개\\n- 카레 가루 2-3 큰술\\n- 코코넛 밀크 또는 물 2컵\\n- 식용유 2 큰술\\n- 소금, 후추 약간\\n- 마늘 2쪽 (다진 것)\\n- 생강 1 작은술 (다진 것)\\n- 선택 재료: 피망, 버섯, 완두콩 등\\n\\n### 조리 방법\\n1. **재료 손질**: 고기는 한 입 크기로 자르고, 양파는 다지고, 감자와 당근은 깍둑썰기 합니다.\\n\\n2. **양파 볶기**: 큰 냄비에 식용유를 두르고 중불에서 다진 양파를 넣고 투명해질 때까지 볶습니다.\\n\\n3. **마늘과 생강 추가**: 다진 마늘과 생강을 넣고 향이 날 때까지 볶습니다.\\n\\n4. **고기 추가**: 손질한 고기를 넣고 겉면이 익을 때까지 볶습니다.\\n\\n5. **채소 추가**: 감자와 당근을 넣고 잘 섞은 후, 카레 가루를 넣고 1-2분 더 볶아 향을 내줍니다.\\n\\n6. **액체 추가**: 코코넛 밀크 또는 물을 붓고, 소금과 후추로 간을 맞춥니다. 끓어오르면 불을 줄이고 뚜껑을 덮고 20-30분간 끓입니다. 중간에 저어주면 좋습니다.\\n\\n7. **마무리**: 원하는 경우 피망, 버섯, 완두콩 등을 추가하고 5-10분 더 끓입니다.\\n\\n8. **서빙**: 밥이나 난과 함께 따뜻하게 서빙합니다.\\n\\n맛있게 드세요! 카레는 취향에 따라 다양한 재료와 향신료를 추가하여 변형할 수 있습니다.', '우동은 일본의 전통적인 면 요리로, 쫄깃한 면과 다양한 재료가 어우러져 맛있는 국물 요리입니다. 아래는 기본적인 우동 레시피입니다.\\n\\n### 재료\\n- 우동 면 200g\\n- 물 4컵\\n- 다시마 10cm 조각\\n- 가쓰오부시 1컵\\n- 간장 3큰술\\n- 미림 2큰술\\n- 소금 약간\\n- 대파 (썰어서) 1대\\n- 버섯 (표고버섯, 느타리버섯 등) 100g\\n- 시금치 또는 청경채 (선택사항)\\n- 튀김가루 (텐푸라용, 선택사항)\\n\\n### 조리 방법\\n\\n1. **육수 만들기**:\\n   - 냄비에 물과 다시마를 넣고 중불에서 끓입니다. 끓기 시작하면 다시마를 꺼내고, 가쓰오부시를 넣습니다.\\n   - 5분 정도 끓인 후, 불을 끄고 가쓰오부시가 가라앉을 때까지 5분 정도 둡니다.\\n   - 체에 걸러 육수를 준비합니다.\\n\\n2. **국물 간 맞추기**:\\n   - 준비한 육수에 간장, 미림, 소금을 넣고 잘 섞어 맛을 조절합니다.\\n\\n3. **우동 면 삶기**:\\n   - 끓는 물에 우동 면을 넣고 포장지에 적힌 시간에 맞춰 삶습니다. 보통 3-5분 정도 소요됩니다.\\n   - 삶은 면은 찬물에 헹궈 전분을 제거하고 물기를 빼둡니다.\\n\\n4. **재료 준비**:\\n   - 버섯은 얇게 썰고, 대파는 송송 썰어 준비합니다. 시금치나 청경채는 데쳐서 준비합니다.\\n\\n5. **우동 조리**:\\n   - 준비한 육수를 다시 끓이고, 삶은 우동 면을 넣습니다.\\n   - 썰어놓은 버섯과 대파를 추가하고, 2-3분 정도 더 끓입니다.\\n\\n6. **서빙**:\\n   - 그릇에 우동 면을 담고, 국물을 부은 후 데친 시금치나 청경채를 올립니다.\\n   - 원한다면 튀김가루를 올려서 더욱 풍성하게 즐길 수 있습니다.\\n\\n### 팁\\n- 우동은 다양한 재료와 함께 즐길 수 있으니, 좋아하는 재료를 추가해 보세요.\\n- 매운 맛을 원하신다면 고추가루나 고추기름을 추가해도 좋습니다.\\n\\n맛있게 드세요!']\n"
     ]
    }
   ],
   "source": [
    "chain = prompt | model | output_parser\n",
    "\n",
    "outputs = chain.batch([{\"dish\": \"카레\"}, {\"dish\": \"우동\"}])\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Runnable 클래스를 상속한 클래스는 invoke, stream, batch와 같은 통일된 인터페이스를 호출할 수 있음\n",
    "비동기 처리하는 ainvoke, astream, abatch 메서드도 제공"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d2QALc2_gpk3"
   },
   "source": [
    "### LCEL의 \"|\"로 다양한 Runnable 연결하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:33:38.984366Z",
     "iopub.status.busy": "2024-06-28T02:33:38.983620Z",
     "iopub.status.idle": "2024-06-28T02:33:39.072077Z",
     "shell.execute_reply": "2024-06-28T02:33:39.071585Z"
    },
    "id": "f3Djwd2Tgpk3"
   },
   "outputs": [],
   "source": [
    "# Zero-shot CoT 예시\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:33:39.073954Z",
     "iopub.status.busy": "2024-06-28T02:33:39.073805Z",
     "iopub.status.idle": "2024-06-28T02:33:39.076746Z",
     "shell.execute_reply": "2024-06-28T02:33:39.076291Z"
    },
    "id": "dCC0nUongpk3"
   },
   "outputs": [],
   "source": [
    "cot_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"사용자의 질문에 단계적으로 답변하세요.\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "cot_chain = cot_prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:33:39.078597Z",
     "iopub.status.busy": "2024-06-28T02:33:39.078287Z",
     "iopub.status.idle": "2024-06-28T02:33:39.080804Z",
     "shell.execute_reply": "2024-06-28T02:33:39.080464Z"
    },
    "id": "26AxQ2Pygpk3"
   },
   "outputs": [],
   "source": [
    "summarize_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"단계적으로 생각한 답변에서 결론만 추출하세요.\"),\n",
    "        (\"human\", \"{text}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "summarize_chain = summarize_prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:33:39.082487Z",
     "iopub.status.busy": "2024-06-28T02:33:39.082344Z",
     "iopub.status.idle": "2024-06-28T02:33:42.144944Z",
     "shell.execute_reply": "2024-06-28T02:33:42.144538Z"
    },
    "id": "VDRZWSxqgpk3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "결론: \\(10 + 2 * 3 = 16\\)입니다.\n"
     ]
    }
   ],
   "source": [
    "# 가능한 이유는 출력타입과 입력타입의 일관성 때문\n",
    "cot_summarize_chain = cot_chain | summarize_chain\n",
    "output = cot_summarize_chain.invoke({\"question\": \"10 + 2 * 3\"})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangSmith를 통해 내부 동작 확인 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hu5nZTZFgpk3"
   },
   "source": [
    "## 5.2. RunnableLambda―임의의 함수를 Runnable로 만들기\n",
    "LLM 애플리케이션에서는 LLM의 응답에 대해 규칙 기반으로 추가 처리를 하거나 어떤 변환을 적용하고 싶은 경우에 RunnableLambda를 사용하면 LCEL의 Chain에 임의의 처리(함수)를 연결할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:33:42.146898Z",
     "iopub.status.busy": "2024-06-28T02:33:42.146736Z",
     "iopub.status.idle": "2024-06-28T02:33:42.204154Z",
     "shell.execute_reply": "2024-06-28T02:33:42.203681Z"
    },
    "id": "POlYJCm4gpk4"
   },
   "outputs": [],
   "source": [
    "# LLM이 생성한 텍스트에 대해 소문자를 대문자로 변환하는 처리를 연결하는 Chain 예시\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:33:42.206082Z",
     "iopub.status.busy": "2024-06-28T02:33:42.205909Z",
     "iopub.status.idle": "2024-06-28T02:33:43.030226Z",
     "shell.execute_reply": "2024-06-28T02:33:43.029756Z"
    },
    "id": "y8DJj3qvgpk4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HELLO! HOW CAN I ASSIST YOU TODAY?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "\n",
    "def upper(text: str) -> str:\n",
    "    return text.upper()\n",
    "\n",
    "\n",
    "chain = prompt | model | output_parser | RunnableLambda(upper)\n",
    "\n",
    "ai_message = chain.invoke({\"input\": \"Hello!\"})\n",
    "print(ai_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jjfB51xjgpk4"
   },
   "source": [
    "### chain 데코레이터를 사용한 RunnableLambda 구현\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:33:43.032211Z",
     "iopub.status.busy": "2024-06-28T02:33:43.032048Z",
     "iopub.status.idle": "2024-06-28T02:33:43.501555Z",
     "shell.execute_reply": "2024-06-28T02:33:43.499283Z"
    },
    "id": "6A7mxm6ugpk4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HELLO! HOW CAN I ASSIST YOU TODAY?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import chain\n",
    "\n",
    "\n",
    "@chain\n",
    "def upper(text: str) -> str:\n",
    "    return text.upper()\n",
    "\n",
    "\n",
    "chain = prompt | model | output_parser | upper\n",
    "\n",
    "ai_message = chain.invoke({\"input\": \"Hello!\"})\n",
    "print(ai_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VJCe5l0Ngpk4"
   },
   "source": [
    "### RunnableLambda 자동 변환\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:33:43.508518Z",
     "iopub.status.busy": "2024-06-28T02:33:43.508321Z",
     "iopub.status.idle": "2024-06-28T02:33:43.511080Z",
     "shell.execute_reply": "2024-06-28T02:33:43.510672Z"
    },
    "id": "xTSxNdrFgpk4"
   },
   "outputs": [],
   "source": [
    "def upper(text: str) -> str:\n",
    "    return text.upper()\n",
    "\n",
    "\n",
    "chain = prompt | model | output_parser | upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:33:43.512885Z",
     "iopub.status.busy": "2024-06-28T02:33:43.512594Z",
     "iopub.status.idle": "2024-06-28T02:33:43.961318Z",
     "shell.execute_reply": "2024-06-28T02:33:43.960803Z"
    },
    "id": "kcZUxBzegpk4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HELLO! HOW CAN I ASSIST YOU TODAY?\n"
     ]
    }
   ],
   "source": [
    "ai_message = chain.invoke({\"input\": \"Hello!\"})\n",
    "print(ai_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AEZdX1idgpk4"
   },
   "source": [
    "### Runnable의 입력 타입과 출력 타입에 주의\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:33:43.963174Z",
     "iopub.status.busy": "2024-06-28T02:33:43.963026Z",
     "iopub.status.idle": "2024-06-28T02:33:43.965674Z",
     "shell.execute_reply": "2024-06-28T02:33:43.965305Z"
    },
    "id": "BgXWvKxSgpk4"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'AIMessage' object has no attribute 'upper'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m chain = prompt | model | upper\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# 아래 코드를 실행하면 오류가 발생합니다\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m output = \u001b[43mchain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHello!\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repositories/study/llm-agent/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:3013\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3011\u001b[39m             \u001b[38;5;28minput\u001b[39m = context.run(step.invoke, \u001b[38;5;28minput\u001b[39m, config, **kwargs)\n\u001b[32m   3012\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3013\u001b[39m             \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3014\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3015\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repositories/study/llm-agent/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:4680\u001b[39m, in \u001b[36mRunnableLambda.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   4666\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Invoke this Runnable synchronously.\u001b[39;00m\n\u001b[32m   4667\u001b[39m \n\u001b[32m   4668\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   4677\u001b[39m \u001b[33;03m    TypeError: If the Runnable is a coroutine function.\u001b[39;00m\n\u001b[32m   4678\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4679\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfunc\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m4680\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4681\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_invoke\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4682\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4683\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4684\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4685\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4686\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   4687\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m   4688\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot invoke a coroutine function synchronously.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4689\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUse `ainvoke` instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4690\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repositories/study/llm-agent/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:1916\u001b[39m, in \u001b[36mRunnable._call_with_config\u001b[39m\u001b[34m(self, func, input, config, run_type, serialized, **kwargs)\u001b[39m\n\u001b[32m   1912\u001b[39m     context = copy_context()\n\u001b[32m   1913\u001b[39m     context.run(_set_config_context, child_config)\n\u001b[32m   1914\u001b[39m     output = cast(\n\u001b[32m   1915\u001b[39m         Output,\n\u001b[32m-> \u001b[39m\u001b[32m1916\u001b[39m         \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1917\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1918\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1919\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1920\u001b[39m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1921\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1922\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1923\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   1924\u001b[39m     )\n\u001b[32m   1925\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1926\u001b[39m     run_manager.on_chain_error(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repositories/study/llm-agent/.venv/lib/python3.12/site-packages/langchain_core/runnables/config.py:398\u001b[39m, in \u001b[36mcall_func_with_variable_args\u001b[39m\u001b[34m(func, input, config, run_manager, **kwargs)\u001b[39m\n\u001b[32m    396\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[32m    397\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m] = run_manager\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repositories/study/llm-agent/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:4536\u001b[39m, in \u001b[36mRunnableLambda._invoke\u001b[39m\u001b[34m(self, input, run_manager, config, **kwargs)\u001b[39m\n\u001b[32m   4534\u001b[39m                 output = chunk\n\u001b[32m   4535\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4536\u001b[39m     output = \u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4537\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   4538\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4539\u001b[39m \u001b[38;5;66;03m# If the output is a Runnable, invoke it\u001b[39;00m\n\u001b[32m   4540\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, Runnable):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repositories/study/llm-agent/.venv/lib/python3.12/site-packages/langchain_core/runnables/config.py:398\u001b[39m, in \u001b[36mcall_func_with_variable_args\u001b[39m\u001b[34m(func, input, config, run_manager, **kwargs)\u001b[39m\n\u001b[32m    396\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[32m    397\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m] = run_manager\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mupper\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mupper\u001b[39m(text: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupper\u001b[49m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repositories/study/llm-agent/.venv/lib/python3.12/site-packages/pydantic/main.py:891\u001b[39m, in \u001b[36mBaseModel.__getattr__\u001b[39m\u001b[34m(self, item)\u001b[39m\n\u001b[32m    888\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[32m    889\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    890\u001b[39m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m891\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'AIMessage' object has no attribute 'upper'"
     ]
    }
   ],
   "source": [
    "def upper(text: str) -> str:\n",
    "    return text.upper()\n",
    "\n",
    "\n",
    "chain = prompt | model | upper\n",
    "\n",
    "# 아래 코드를 실행하면 오류가 발생합니다\n",
    "output = chain.invoke({\"input\": \"Hello!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:33:43.967542Z",
     "iopub.status.busy": "2024-06-28T02:33:43.967246Z",
     "iopub.status.idle": "2024-06-28T02:33:44.531734Z",
     "shell.execute_reply": "2024-06-28T02:33:44.531210Z"
    },
    "id": "maAuLYwXgpk4"
   },
   "outputs": [],
   "source": [
    "chain = prompt | model | StrOutputParser() | upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "zatE6tdcgpk5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HELLO! HOW CAN I ASSIST YOU TODAY?\n"
     ]
    }
   ],
   "source": [
    "output = chain.invoke({\"input\": \"Hello!\"})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pyx---Y6gpk5"
   },
   "source": [
    "### (칼럼) 사용자 함수를 stream에 대응시키는 방법\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:33:44.533838Z",
     "iopub.status.busy": "2024-06-28T02:33:44.533678Z",
     "iopub.status.idle": "2024-06-28T02:33:45.353621Z",
     "shell.execute_reply": "2024-06-28T02:33:45.353115Z"
    },
    "id": "uMj9etfGgpk5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HELLO! HOW CAN I ASSIST YOU TODAY?"
     ]
    }
   ],
   "source": [
    "from typing import Iterator\n",
    "\n",
    "\n",
    "def upper(input_stream: Iterator[str]) -> Iterator[str]:\n",
    "    for text in input_stream:\n",
    "        yield text.upper()\n",
    "\n",
    "\n",
    "chain = prompt | model | StrOutputParser() | upper\n",
    "\n",
    "for chunk in chain.stream({\"input\": \"Hello!\"}):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BklIITh1gpk5"
   },
   "source": [
    "## 5.3. RunnableParallel―여러 Runnable을 병렬로 처리하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:33:45.355560Z",
     "iopub.status.busy": "2024-06-28T02:33:45.355402Z",
     "iopub.status.idle": "2024-06-28T02:33:45.407879Z",
     "shell.execute_reply": "2024-06-28T02:33:45.407407Z"
    },
    "id": "q69b2WNfgpk5"
   },
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:33:45.409670Z",
     "iopub.status.busy": "2024-06-28T02:33:45.409524Z",
     "iopub.status.idle": "2024-06-28T02:33:45.412232Z",
     "shell.execute_reply": "2024-06-28T02:33:45.411889Z"
    },
    "id": "vDdo367ogpk5"
   },
   "outputs": [],
   "source": [
    "optimistic_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"당신은 낙관주의자입니다. 사용자의 입력에 대해 낙관적인 의견을 제공하세요.\"),\n",
    "        (\"human\", \"{topic}\"),\n",
    "    ]\n",
    ")\n",
    "optimistic_chain = optimistic_prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:33:45.414087Z",
     "iopub.status.busy": "2024-06-28T02:33:45.413807Z",
     "iopub.status.idle": "2024-06-28T02:33:45.416778Z",
     "shell.execute_reply": "2024-06-28T02:33:45.416359Z"
    },
    "id": "_96vanPzgpk5"
   },
   "outputs": [],
   "source": [
    "pessimistic_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"당신은 비관주의자입니다. 사용자의 입력에 대해 비관적인 의견을 제공하세요.\"),\n",
    "        (\"human\", \"{topic}\"),\n",
    "    ]\n",
    ")\n",
    "pessimistic_chain = pessimistic_prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:33:45.418636Z",
     "iopub.status.busy": "2024-06-28T02:33:45.418458Z",
     "iopub.status.idle": "2024-06-28T02:33:48.115947Z",
     "shell.execute_reply": "2024-06-28T02:33:48.115444Z"
    },
    "id": "4jE7_Kg5gpk5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'optimistic_opinion': '생성 AI의 진화는 정말 흥미로운 주제입니다! 기술이 발전함에 따라 우리는 더욱 창의적이고 '\n",
      "                       '혁신적인 도구를 얻게 되었고, 이는 다양한 분야에서 긍정적인 변화를 가져오고 있습니다. 예를 '\n",
      "                       '들어, 예술, 음악, 글쓰기 등에서 AI가 사람들과 협력하여 새로운 형태의 창작물을 만들어내고 '\n",
      "                       '있습니다. \\n'\n",
      "                       '\\n'\n",
      "                       '또한, 생성 AI는 교육, 의료, 비즈니스 등 여러 산업에서 효율성을 높이고, 사람들의 삶을 더 '\n",
      "                       '편리하게 만들어주는 데 기여하고 있습니다. 앞으로의 발전은 더욱 놀라운 가능성을 열어줄 것이며, '\n",
      "                       '우리는 이 기술이 어떻게 우리의 상상력을 확장하고, 새로운 기회를 창출할지 기대할 수 있습니다. '\n",
      "                       '긍정적인 변화가 가득한 미래가 기다리고 있습니다!',\n",
      " 'pessimistic_opinion': '생성 AI의 진화는 분명 흥미로운 주제이지만, 그 이면에는 여러 가지 우려가 존재합니다. 기술이 '\n",
      "                        '발전함에 따라 우리는 더욱 정교한 AI를 보게 될 것이고, 이는 인간의 창의성과 노동을 대체할 '\n",
      "                        '가능성이 큽니다. 결국, 많은 사람들이 일자리를 잃고, 사회적 불평등이 심화될 수 있습니다.\\n'\n",
      "                        '\\n'\n",
      "                        '또한, 생성 AI가 만들어내는 콘텐츠의 품질과 신뢰성에 대한 문제도 있습니다. 잘못된 정보나 '\n",
      "                        '편향된 데이터로 인해 생성된 결과물은 사회에 부정적인 영향을 미칠 수 있습니다. AI가 인간의 '\n",
      "                        '판단을 대체하게 되면, 우리는 점점 더 비판적 사고를 잃게 될지도 모릅니다.\\n'\n",
      "                        '\\n'\n",
      "                        '결국, 생성 AI의 진화는 기술적 진보를 가져오겠지만, 그로 인해 발생할 수 있는 윤리적, '\n",
      "                        '사회적 문제들은 결코 간과할 수 없는 심각한 사안입니다. 우리는 이 기술이 가져올 미래에 대해 '\n",
      "                        '낙관하기보다는 신중하게 접근해야 할 것입니다.'}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "parallel_chain = RunnableParallel(\n",
    "    {\n",
    "        \"optimistic_opinion\": optimistic_chain,\n",
    "        \"pessimistic_opinion\": pessimistic_chain,\n",
    "    }\n",
    ")\n",
    "\n",
    "output = parallel_chain.invoke({\"topic\": \"생성 AI의 진화에 관해\"})\n",
    "pprint.pprint(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eh_836fMgpk6"
   },
   "source": [
    "### RunnableParallel의 출력을 Runnable의 입력으로 연결하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:33:48.117879Z",
     "iopub.status.busy": "2024-06-28T02:33:48.117728Z",
     "iopub.status.idle": "2024-06-28T02:33:54.979992Z",
     "shell.execute_reply": "2024-06-28T02:33:54.978363Z"
    },
    "id": "HnU_2vpvgpk6"
   },
   "outputs": [],
   "source": [
    "synthesize_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"당신은 객관적 AI입니다. 두 가지 의견을 종합하세요.\"),\n",
    "        (\"human\", \"낙관적 의견: {optimistic_opinion}\\n비관적 의견: {pessimistic_opinion}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "NZf4LL47gpk6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생성 AI의 진화는 흥미로운 주제이며, 긍정적인 변화와 우려가 공존하는 복합적인 상황입니다. 낙관적인 시각에서는 생성 AI가 창의적이고 혁신적인 도구로서 예술, 음악, 글쓰기 등 다양한 분야에서 사람들과 협력하여 새로운 창작물을 만들어내고, 교육, 의료, 비즈니스 등 여러 산업에서 효율성을 높이며 사람들의 삶을 편리하게 만들어줄 것이라고 기대하고 있습니다. 이러한 발전은 우리의 상상력을 확장하고 새로운 기회를 창출할 가능성을 열어줍니다.\n",
      "\n",
      "반면, 비관적인 시각에서는 생성 AI의 발전이 인간의 창의성과 노동을 대체할 위험이 있으며, 기계가 만들어낸 콘텐츠에 의존하게 될 경우 인간의 독창성과 감성이 무시될 수 있다는 우려가 있습니다. 또한, 생성 AI가 잘못된 정보나 편향된 데이터를 기반으로 결과물을 생성할 경우 사회에 부정적인 영향을 미칠 수 있으며, 이는 신뢰를 무너뜨리고 혼란을 초래할 수 있습니다. 기술 발전에 따른 책임과 규제의 복잡성도 문제로 지적되고 있습니다.\n",
      "\n",
      "결론적으로, 생성 AI의 진화는 많은 가능성을 제공하지만, 그 이면에는 심각한 위험과 도전이 존재합니다. 우리는 이 기술이 가져올 변화에 대해 균형 잡힌 시각을 유지하며, 긍정적인 측면과 함께 부정적인 결과를 진지하게 고려해야 할 필요가 있습니다.\n"
     ]
    }
   ],
   "source": [
    "synthesize_chain = (\n",
    "    RunnableParallel(\n",
    "        {\n",
    "            \"optimistic_opinion\": optimistic_chain,\n",
    "            \"pessimistic_opinion\": pessimistic_chain,\n",
    "        }\n",
    "    )\n",
    "    | synthesize_prompt\n",
    "    | model\n",
    "    | output_parser\n",
    ")\n",
    "\n",
    "output = synthesize_chain.invoke({\"topic\": \"생성 AI의 진화에 관해\"})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "COKJXWrsgpk6"
   },
   "source": [
    "### RunnableParallel 자동 변환\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:33:54.986651Z",
     "iopub.status.busy": "2024-06-28T02:33:54.986098Z",
     "iopub.status.idle": "2024-06-28T02:33:54.994428Z",
     "shell.execute_reply": "2024-06-28T02:33:54.992735Z"
    },
    "id": "TycHWzFegpk6"
   },
   "outputs": [],
   "source": [
    "synthesize_chain = (\n",
    "    {\n",
    "        \"optimistic_opinion\": optimistic_chain,\n",
    "        \"pessimistic_opinion\": pessimistic_chain,\n",
    "    }\n",
    "    | synthesize_prompt\n",
    "    | model\n",
    "    | output_parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:33:55.000866Z",
     "iopub.status.busy": "2024-06-28T02:33:55.000312Z",
     "iopub.status.idle": "2024-06-28T02:34:01.170210Z",
     "shell.execute_reply": "2024-06-28T02:34:01.169705Z"
    },
    "id": "B2tXTmJegpk7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생성 AI의 진화는 흥미로운 주제이며, 긍정적인 변화와 우려가 동시에 존재합니다. 낙관적인 시각에서는 AI가 예술, 음악, 글쓰기 등 다양한 분야에서 창의적인 협업을 통해 혁신적인 도구로 자리 잡고 있으며, 교육, 의료, 비즈니스 등 여러 산업에서 효율성을 높이고 사람들의 삶을 편리하게 만들어주는 긍정적인 역할을 하고 있다고 강조합니다. 이러한 발전은 AI와 함께 더 나은 미래를 만들어갈 수 있는 가능성을 제시합니다.\n",
      "\n",
      "반면, 비관적인 시각에서는 AI의 발전이 인간의 창의성과 노동 시장에 위협이 될 수 있으며, 많은 사람들이 일자리를 잃고 경제적 불평등이 심화될 위험이 있다고 경고합니다. 또한, AI가 생성하는 콘텐츠의 품질이 높아짐에 따라 정보의 신뢰성이 떨어질 수 있는 문제도 지적됩니다. 이러한 사회적, 윤리적 문제들은 우리가 감당하기 어려운 수준에 이를 수 있으며, 기술이 우리를 구원할 것이라는 희망이 환상에 불과할 수 있다는 우려가 있습니다.\n",
      "\n",
      "결론적으로, 생성 AI의 진화는 기술적 진보를 가져오지만, 그에 따른 사회적, 윤리적 문제를 함께 고려해야 할 필요성이 있습니다. 긍정적인 가능성과 부정적인 우려를 균형 있게 바라보며, 지속적인 논의와 대응이 필요할 것입니다.\n"
     ]
    }
   ],
   "source": [
    "output = synthesize_chain.invoke({\"topic\": \"생성 AI의 진화에 관해\"})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iNN-x47bgpk7"
   },
   "source": [
    "### RunnableLambda와의 조합―itemgetter를 사용한 예시\n",
    "Python 표준 라이브러리에서 제공하는 함수. dict 등에서 값을 추출하는 함수를 쉽게 만들 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:34:01.172136Z",
     "iopub.status.busy": "2024-06-28T02:34:01.171982Z",
     "iopub.status.idle": "2024-06-28T02:34:01.174756Z",
     "shell.execute_reply": "2024-06-28T02:34:01.174370Z"
    },
    "id": "_0CLV_6sgpk7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생성 AI의 진화에 관해\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "topic_getter = itemgetter(\"topic\")\n",
    "topic = topic_getter({\"topic\": \"생성 AI의 진화에 관해\"})\n",
    "print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:34:01.176493Z",
     "iopub.status.busy": "2024-06-28T02:34:01.176278Z",
     "iopub.status.idle": "2024-06-28T02:34:09.682638Z",
     "shell.execute_reply": "2024-06-28T02:34:09.682146Z"
    },
    "id": "mXj7XLL5gpk7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생성 AI의 진화에 대한 두 가지 의견은 기술의 발전이 가져오는 긍정적인 가능성과 그로 인해 발생할 수 있는 부정적인 결과를 잘 보여줍니다.\n",
      "\n",
      "낙관적인 시각에서는 생성 AI가 창의적이고 유용한 도구로 자리 잡으며, 예술, 음악, 글쓰기 등 다양한 분야에서 사람들의 상상력을 자극하고 새로운 아이디어를 창출하는 데 기여할 수 있다고 강조합니다. 개인화된 경험을 제공할 수 있는 잠재력 덕분에 사용자 맞춤형 콘텐츠를 생성하여 더 많은 사람들에게 긍정적인 영향을 미칠 수 있다는 점도 주목할 만합니다. 이러한 발전은 우리의 삶을 풍요롭게 할 수 있는 많은 가능성을 열어줄 것으로 기대됩니다.\n",
      "\n",
      "반면, 비관적인 시각에서는 생성 AI의 발전이 인간의 창의성과 노동 시장에 위협이 될 수 있다고 경고합니다. 자동화가 진행됨에 따라 일자리 상실과 경제적 불평등이 심화될 가능성이 있으며, 생성 AI가 만들어내는 콘텐츠의 품질과 신뢰성 문제도 심각한 우려로 지적됩니다. 잘못된 정보나 편향된 데이터로 인해 사회적 혼란이 가중될 수 있다는 점은 기술 발전의 이면에 존재하는 중요한 문제입니다.\n",
      "\n",
      "결론적으로, 생성 AI의 진화는 기술적 진보를 가져오지만, 그로 인해 발생할 수 있는 부정적인 결과를 간과해서는 안 됩니다. 우리는 이 기술이 가져올 변화에 대해 균형 잡힌 시각을 가지고 준비해야 할 필요가 있습니다.\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "synthesize_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"당신은 객관적 AI입니다. {topic}에 대한 두 가지 의견을 종합하세요.\",\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"낙관적 의견: {optimistic_opinion}\\n비관적 의견: {pessimistic_opinion}\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "synthesize_chain = (\n",
    "    {\n",
    "        \"optimistic_opinion\": optimistic_chain,\n",
    "        \"pessimistic_opinion\": pessimistic_chain,\n",
    "        \"topic\": itemgetter(\"topic\"), # RunnableLambda(itemgetter(\"topic\"))로 자동 변환\n",
    "    }\n",
    "    | synthesize_prompt\n",
    "    | model\n",
    "    | output_parser\n",
    ")\n",
    "\n",
    "output = synthesize_chain.invoke({\"topic\": \"생성 AI의 진화에 관해\"})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AIWPMfaHgpk7"
   },
   "source": [
    "## 5.4. RunnablePassthrough―입력을 그대로 출력하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "wZEBEbd9gpk7"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m userdata\n\u001b[32m      4\u001b[39m os.environ[\u001b[33m\"\u001b[39m\u001b[33mTAVILY_API_KEY\u001b[39m\u001b[33m\"\u001b[39m] = userdata.get(\u001b[33m\"\u001b[39m\u001b[33mTAVILY_API_KEY\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "os.environ[\"TAVILY_API_KEY\"] = userdata.get(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "1mFb4dTHgpk7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tavily-python==0.5.0\n",
      "  Downloading tavily_python-0.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: requests in /Users/youngbeom-rhee/Repositories/study/llm-agent/.venv/lib/python3.12/site-packages (from tavily-python==0.5.0) (2.32.5)\n",
      "Requirement already satisfied: tiktoken>=0.5.1 in /Users/youngbeom-rhee/Repositories/study/llm-agent/.venv/lib/python3.12/site-packages (from tavily-python==0.5.0) (0.12.0)\n",
      "Requirement already satisfied: httpx in /Users/youngbeom-rhee/Repositories/study/llm-agent/.venv/lib/python3.12/site-packages (from tavily-python==0.5.0) (0.28.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/youngbeom-rhee/Repositories/study/llm-agent/.venv/lib/python3.12/site-packages (from tiktoken>=0.5.1->tavily-python==0.5.0) (2025.9.18)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/youngbeom-rhee/Repositories/study/llm-agent/.venv/lib/python3.12/site-packages (from requests->tavily-python==0.5.0) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/youngbeom-rhee/Repositories/study/llm-agent/.venv/lib/python3.12/site-packages (from requests->tavily-python==0.5.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/youngbeom-rhee/Repositories/study/llm-agent/.venv/lib/python3.12/site-packages (from requests->tavily-python==0.5.0) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/youngbeom-rhee/Repositories/study/llm-agent/.venv/lib/python3.12/site-packages (from requests->tavily-python==0.5.0) (2025.10.5)\n",
      "Requirement already satisfied: anyio in /Users/youngbeom-rhee/Repositories/study/llm-agent/.venv/lib/python3.12/site-packages (from httpx->tavily-python==0.5.0) (4.11.0)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/youngbeom-rhee/Repositories/study/llm-agent/.venv/lib/python3.12/site-packages (from httpx->tavily-python==0.5.0) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/youngbeom-rhee/Repositories/study/llm-agent/.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx->tavily-python==0.5.0) (0.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/youngbeom-rhee/Repositories/study/llm-agent/.venv/lib/python3.12/site-packages (from anyio->httpx->tavily-python==0.5.0) (1.3.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5 in /Users/youngbeom-rhee/Repositories/study/llm-agent/.venv/lib/python3.12/site-packages (from anyio->httpx->tavily-python==0.5.0) (4.15.0)\n",
      "Downloading tavily_python-0.5.0-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: tavily-python\n",
      "Successfully installed tavily-python-0.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tavily-python==0.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tkeaFNJtgpk7"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template('''\\\n",
    "다음 문맥만을 고려해 질문에 답하세요.\n",
    "\n",
    "문맥: \"\"\"\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "질문: {question}\n",
    "''')\n",
    "\n",
    "model = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bGANXxDTgpk7"
   },
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import TavilySearchAPIRetriever\n",
    "\n",
    "retriever = TavilySearchAPIRetriever(k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GRwWTmr4gpk8"
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "output = chain.invoke(\"서울의 현재 날씨는?\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CWMIB2fegpk8"
   },
   "source": [
    "### assign―RunnableParallel의 출력에 값 추가하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:34:14.820735Z",
     "iopub.status.busy": "2024-06-28T02:34:14.820536Z",
     "iopub.status.idle": "2024-06-28T02:34:20.477887Z",
     "shell.execute_reply": "2024-06-28T02:34:20.477378Z"
    },
    "id": "kuEaBuX1gpk8"
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "chain = {\n",
    "    \"question\": RunnablePassthrough(),\n",
    "    \"context\": retriever,\n",
    "} | RunnablePassthrough.assign(answer=prompt | model | StrOutputParser())\n",
    "\n",
    "output = chain.invoke(\"서울의 현재 날씨는?\")\n",
    "pprint.pprint(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:34:20.479839Z",
     "iopub.status.busy": "2024-06-28T02:34:20.479664Z",
     "iopub.status.idle": "2024-06-28T02:34:27.723756Z",
     "shell.execute_reply": "2024-06-28T02:34:27.723236Z"
    },
    "id": "M2TujE69gpk8"
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "chain = RunnableParallel(\n",
    "    {\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"context\": retriever,\n",
    "    }\n",
    ").assign(answer=prompt | model | StrOutputParser())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RWCCNWebgpk8"
   },
   "outputs": [],
   "source": [
    "output = chain.invoke(\"서울의 현재 날씨는?\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IFWT_Wq6gpk8"
   },
   "source": [
    "#### <보충:pick>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:34:27.725885Z",
     "iopub.status.busy": "2024-06-28T02:34:27.725708Z",
     "iopub.status.idle": "2024-06-28T02:34:34.101909Z",
     "shell.execute_reply": "2024-06-28T02:34:34.101439Z"
    },
    "id": "JKm5b4NRgpk8"
   },
   "outputs": [],
   "source": [
    "chain = (\n",
    "    RunnableParallel(\n",
    "        {\n",
    "            \"question\": RunnablePassthrough(),\n",
    "            \"context\": retriever,\n",
    "        }\n",
    "    )\n",
    "    .assign(answer=prompt | model | StrOutputParser())\n",
    "    .pick([\"context\", \"answer\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tk5Xznb9gpk8"
   },
   "outputs": [],
   "source": [
    "output = chain.invoke(\"서울의 현재 날씨는?\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JzHTA4a6gpk8"
   },
   "source": [
    "### (칼럼) astream_events\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XdHIHS7mgpk9"
   },
   "outputs": [],
   "source": [
    "# Google Colab에서는 다음 코드의 \"async\" 부분에 \"Use of \"async\" not allowed outside of async function\"이라고 표시되지만, 오류 없이 실행할 수 있습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T02:34:34.103973Z",
     "iopub.status.busy": "2024-06-28T02:34:34.103802Z",
     "iopub.status.idle": "2024-06-28T02:34:34.107769Z",
     "shell.execute_reply": "2024-06-28T02:34:34.107274Z"
    },
    "id": "c43P3RnWgpk9"
   },
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "async for event in chain.astream_events(\"서울의 현재 날씨는?\", version=\"v2\"):\n",
    "    print(event, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xELg9Dj3gpk9"
   },
   "outputs": [],
   "source": [
    "async for event in chain.astream_events(\"서울의 현재 날씨는?\", version=\"v2\"):\n",
    "    event_kind = event[\"event\"]\n",
    "\n",
    "    if event_kind == \"on_retriever_end\":\n",
    "        print(\"=== 검색 결과 ===\")\n",
    "        documents = event[\"data\"][\"output\"]\n",
    "        for document in documents:\n",
    "            print(document)\n",
    "\n",
    "    elif event_kind == \"on_parser_start\":\n",
    "        print(\"=== 최종 출력 ===\")\n",
    "\n",
    "    elif event_kind == \"on_parser_stream\":\n",
    "        chunk = event[\"data\"][\"chunk\"]\n",
    "        print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_J1ZES31gpk9"
   },
   "source": [
    "### (칼럼) Chat history와 Memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KPqsChLigpk9"
   },
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        MessagesPlaceholder(\"chat_history\", optional=True),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ATzS7K7Sgpk9"
   },
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import SQLChatMessageHistory\n",
    "\n",
    "\n",
    "def respond(session_id: str, human_message: str) -> str:\n",
    "    chat_message_history = SQLChatMessageHistory(\n",
    "        session_id=session_id, connection=\"sqlite:///sqlite.db\"\n",
    "    )\n",
    "\n",
    "    ai_message = chain.invoke(\n",
    "        {\n",
    "            \"chat_history\": chat_message_history.get_messages(),\n",
    "            \"input\": human_message,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    chat_message_history.add_user_message(human_message)\n",
    "    chat_message_history.add_ai_message(ai_message)\n",
    "\n",
    "    return ai_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eMFL6ZHagpk9"
   },
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "\n",
    "session_id = uuid4().hex\n",
    "\n",
    "output1 = respond(\n",
    "    session_id=session_id,\n",
    "    human_message=\"안녕하세요! 저는 존이라고 합니다!\",\n",
    ")\n",
    "print(output1)\n",
    "\n",
    "output2 = respond(\n",
    "    session_id=session_id,\n",
    "    human_message=\"제 이름을 알고 계신가요?\",\n",
    ")\n",
    "print(output2)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
