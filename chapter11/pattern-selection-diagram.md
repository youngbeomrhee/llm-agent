# Agent Design Pattern Selection Diagrams

## 1. íŒ¨í„´ ì„ íƒ ì˜ì‚¬ê²°ì • í”Œë¡œìš°ì°¨íŠ¸

```mermaid
flowchart TD
    Start([Agent ì„¤ê³„ ì‹œì‘])

    Start --> Q1{í™˜ê²½ ì •ë³´<br/>ìº¡ì²˜ í•„ìš”?}

    Q1 -->|Yes| ProactiveGoal[Proactive Goal Creator<br/>+ ì ‘ê·¼ì„±, ëª©í‘œ íƒìƒ‰<br/>- ì˜¤ë²„í—¤ë“œ]
    Q1 -->|No| PassiveGoal[Passive Goal Creator<br/>+ íš¨ìœ¨ì„±, ìƒí˜¸ì‘ìš©<br/>- ì¶”ë¡  ë¶ˆí™•ì‹¤ì„±]

    ProactiveGoal --> Q2{í”„ë¡¬í”„íŠ¸<br/>ìµœì í™” í•„ìš”?}
    PassiveGoal --> Q2

    Q2 -->|Yes| PromptOpt[Prompt/Response Optimiser<br/>+ í‘œì¤€í™”, ëª©í‘œ ì •ë ¬<br/>- ìœ ì§€ë³´ìˆ˜ ì˜¤ë²„í—¤ë“œ]
    Q2 -->|No| Q3{ì™¸ë¶€ ë°ì´í„°<br/>ì €ì¥ì†Œ ì‚¬ìš©?}

    PromptOpt --> Q3

    Q3 -->|Yes| RAG[Retrieval Augmented Generation<br/>+ ì§€ì‹ ê²€ìƒ‰, ë°ì´í„° í”„ë¼ì´ë²„ì‹œ<br/>- ìœ ì§€ë³´ìˆ˜ ì˜¤ë²„í—¤ë“œ]
    Q3 -->|No| Q4{FM ì¿¼ë¦¬<br/>íšŸìˆ˜?}

    RAG --> Q4

    Q4 -->|í•œ ë²ˆ| OneShot[One-shot Model Querying<br/>+ ë¹„ìš© íš¨ìœ¨ì„±, ë‹¨ìˆœì„±<br/>- ê³¼ë„í•œ ë‹¨ìˆœí™”]
    Q4 -->|ì—¬ëŸ¬ ë²ˆ| Incremental[Incremental Model Querying<br/>+ ì¶”ë¡  í™•ì‹¤ì„±, ì„¤ëª… ê°€ëŠ¥ì„±<br/>- ì˜¤ë²„í—¤ë“œ]

    OneShot --> Q5{ê³„íš ìƒì„±<br/>ë°©ì‹?}
    Incremental --> Q5

    Q5 -->|ë‹¨ì¼ ê²½ë¡œ| SinglePath[Single-path Plan Generator<br/>+ íš¨ìœ¨ì„±, ì¼ê´€ì„±<br/>- ìœ ì—°ì„± ë¶€ì¡±]
    Q5 -->|ë‹¤ì¤‘ ê²½ë¡œ| MultiPath[Multi-path Plan Generator<br/>+ ì¸ê°„ ì„ í˜¸ ì •ë ¬, í¬ê´„ì„±<br/>- ì˜¤ë²„í—¤ë“œ]

    SinglePath --> Q6{ê³„íš ê²€í† <br/>í•„ìš”?}
    MultiPath --> Q6

    Q6 -->|No| Q7{FM ì…ì¶œë ¥<br/>ì œì–´ í•„ìš”?}
    Q6 -->|Yes| ReflectionType{ê²€í†  ì£¼ì²´?}

    ReflectionType -->|ìì²´| SelfRef[Self-reflection<br/>+ ì§€ì†ì  ê°œì„ , íš¨ìœ¨ì„±<br/>- ì¶”ë¡  ë¶ˆí™•ì‹¤ì„±]
    ReflectionType -->|ë‹¤ë¥¸ ì—ì´ì „íŠ¸| CrossRef[Cross-reflection<br/>+ í™•ì¥ì„±, í¬ê´„ì„±<br/>- ë³µì¡í•œ ì±…ì„ì„±]
    ReflectionType -->|ì¸ê°„| HumanRef[Human Reflection<br/>+ ì¸ê°„ ì„ í˜¸ ì •ë ¬, ì´ì˜ì œê¸°<br/>- ê³µì •ì„± ë³´ì¡´ ì–´ë ¤ì›€]

    SelfRef --> Q7
    CrossRef --> CoopType{í˜‘ë ¥ ë°©ì‹?}
    HumanRef --> Q7

    CoopType -->|íˆ¬í‘œ| Voting[Voting-based Cooperation<br/>+ ê³µì •ì„±, ì±…ì„ì„±<br/>- ì¤‘ì•™í™”]
    CoopType -->|ì—­í• | RoleBased[Role-based Cooperation<br/>+ ì—…ë¬´ ë¶„ë‹´, í™•ì¥ì„±<br/>- ì˜¤ë²„í—¤ë“œ]
    CoopType -->|í† ë¡ | Debate[Debate-based Cooperation<br/>+ ì ì‘ì„±, ì„¤ëª… ê°€ëŠ¥ì„±<br/>- ì œí•œëœ ëŠ¥ë ¥]

    Voting --> Q7
    RoleBased --> Q7
    Debate --> Q7

    Q7 -->|Yes| Guardrails[Multimodal Guardrails<br/>+ ê²¬ê³ ì„±, ì•ˆì „ì„±<br/>- ì„¤ëª… ê°€ëŠ¥ì„± ë¶€ì¡±]
    Q7 -->|No| Q8{ì™¸ë¶€ ë„êµ¬/<br/>ì—ì´ì „íŠ¸ ì‚¬ìš©?}

    Guardrails --> Q8

    Q8 -->|Yes| Registry[Tool/Agent Registry<br/>+ ê²€ìƒ‰ ê°€ëŠ¥ì„±, íš¨ìœ¨ì„±<br/>- ì¤‘ì•™í™”]
    Q8 -->|No| Q9{ì„±ëŠ¥ í‰ê°€<br/>í•„ìš”?}

    Registry --> Adapter[Agent Adapter<br/>+ ìƒí˜¸ìš´ìš©ì„±, ì ì‘ì„±<br/>- ìœ ì§€ë³´ìˆ˜ ì˜¤ë²„í—¤ë“œ]

    Adapter --> Q9

    Q9 -->|Yes| Evaluator[Agent Evaluator<br/>+ ê¸°ëŠ¥ ì í•©ì„±, ì ì‘ì„±<br/>- ë©”íŠ¸ë¦­ ì •ëŸ‰í™” ì–´ë ¤ì›€]
    Q9 -->|No| End([ì„¤ê³„ ì™„ë£Œ])

    Evaluator --> End

    style ProactiveGoal fill:#e1f5e1
    style PassiveGoal fill:#e1f5e1
    style PromptOpt fill:#fff4e1
    style RAG fill:#e1f0ff
    style OneShot fill:#ffe1f0
    style Incremental fill:#ffe1f0
    style SinglePath fill:#f0e1ff
    style MultiPath fill:#f0e1ff
    style SelfRef fill:#ffe1e1
    style CrossRef fill:#ffe1e1
    style HumanRef fill:#ffe1e1
    style Voting fill:#e1ffe1
    style RoleBased fill:#e1ffe1
    style Debate fill:#e1ffe1
    style Guardrails fill:#fff5e1
    style Registry fill:#e1f5ff
    style Adapter fill:#e1f5ff
    style Evaluator fill:#f5e1ff
```

## 2. íŒ¨í„´ ì¹´í…Œê³ ë¦¬ ë° ê´€ê³„ë„

```mermaid
graph TB
    subgraph Goal_Creation[ëª©í‘œ ìƒì„±]
        P1[Passive Goal Creator]
        P2[Proactive Goal Creator]
    end

    subgraph Context_Management[ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬]
        P3[Prompt/Response Optimiser]
        P4[RAG]
    end

    subgraph Model_Querying[ëª¨ë¸ ì¿¼ë¦¬]
        P5[One-shot Model Querying]
        P6[Incremental Model Querying]
    end

    subgraph Plan_Generation[ê³„íš ìƒì„±]
        P7[Single-path Plan Generator]
        P8[Multi-path Plan Generator]
    end

    subgraph Reflection[ë°˜ì˜/ê²€í† ]
        P9[Self-reflection]
        P10[Cross-reflection]
        P11[Human Reflection]
    end

    subgraph Multi_Agent[ë‹¤ì¤‘ ì—ì´ì „íŠ¸ í˜‘ë ¥]
        P12[Voting-based Cooperation]
        P13[Role-based Cooperation]
        P14[Debate-based Cooperation]
    end

    subgraph Safety_Integration[ì•ˆì „ì„± ë° í†µí•©]
        P15[Multimodal Guardrails]
        P16[Tool/Agent Registry]
        P17[Agent Adapter]
    end

    subgraph Evaluation[í‰ê°€]
        P18[Agent Evaluator]
    end

    P1 -.ëŒ€ì•ˆ.-> P2
    P1 --> P3
    P2 --> P3
    P2 --> P15

    P3 --> P9
    P3 --> P10
    P3 --> P11
    P3 --> P17

    P4 -.ë³´ì™„.-> P1
    P4 -.ë³´ì™„.-> P2
    P4 -.ë³´ì™„.-> P3

    P5 -.ëŒ€ì•ˆ.-> P6
    P5 --> P7
    P5 --> P15

    P6 --> P8
    P6 --> P9
    P6 --> P11
    P6 --> P15

    P7 -.ëŒ€ì•ˆ.-> P8
    P7 --> P9

    P8 --> P11

    P9 --> P3
    P9 --> P6
    P9 --> P7

    P10 --> P3
    P10 --> P12
    P10 --> P13
    P10 --> P14
    P10 --> P16

    P11 --> P3
    P11 --> P8
    P11 --> P6

    P12 -.ëŒ€ì•ˆ.-> P13
    P12 -.ëŒ€ì•ˆ.-> P14
    P12 --> P16

    P13 -.ë³´ì™„.-> P12
    P13 -.ë³´ì™„.-> P14
    P13 --> P16

    P14 -.ë³´ì™„.-> P12
    P14 -.ë³´ì™„.-> P13
    P14 --> P16

    P15 --> P2
    P15 --> P5
    P15 --> P6

    P16 --> P10
    P16 --> P12
    P16 --> P13
    P16 --> P14
    P16 --> P17

    P17 --> P3
    P17 --> P16

    P18 -.í‰ê°€.-> P1
    P18 -.í‰ê°€.-> P2
    P18 -.í‰ê°€.-> P3
    P18 -.í‰ê°€.-> P9
    P18 -.í‰ê°€.-> P15

    style P1 fill:#e1f5e1
    style P2 fill:#e1f5e1
    style P3 fill:#fff4e1
    style P4 fill:#e1f0ff
    style P5 fill:#ffe1f0
    style P6 fill:#ffe1f0
    style P7 fill:#f0e1ff
    style P8 fill:#f0e1ff
    style P9 fill:#ffe1e1
    style P10 fill:#ffe1e1
    style P11 fill:#ffe1e1
    style P12 fill:#e1ffe1
    style P13 fill:#e1ffe1
    style P14 fill:#e1ffe1
    style P15 fill:#fff5e1
    style P16 fill:#e1f5ff
    style P17 fill:#e1f5ff
    style P18 fill:#f5e1ff
```

## 3. íŒ¨í„´ ê°„ ê´€ê³„ íƒ€ì… (ERD ìŠ¤íƒ€ì¼)

```mermaid
erDiagram
    GOAL_CREATION ||--|| CONTEXT_MANAGEMENT : enhances
    GOAL_CREATION ||--o{ REFLECTION : receives_feedback
    GOAL_CREATION }o--|| SAFETY : protected_by

    CONTEXT_MANAGEMENT ||--|| MODEL_QUERYING : provides_input
    CONTEXT_MANAGEMENT ||--o{ REFLECTION : optimizes_with
    CONTEXT_MANAGEMENT ||--o{ INTEGRATION : interfaces_with

    MODEL_QUERYING ||--|| PLAN_GENERATION : generates
    MODEL_QUERYING ||--o{ REFLECTION : supports
    MODEL_QUERYING }o--|| SAFETY : controlled_by

    PLAN_GENERATION ||--o{ REFLECTION : reviewed_by
    PLAN_GENERATION ||--o{ EXECUTION : executed_by

    REFLECTION ||--o{ MULTI_AGENT : coordinates
    REFLECTION ||--|| CONTEXT_MANAGEMENT : improves

    MULTI_AGENT ||--|| INTEGRATION : discovers_via
    MULTI_AGENT }o--|| REFLECTION : provides_feedback

    SAFETY ||--o{ GOAL_CREATION : filters
    SAFETY ||--o{ MODEL_QUERYING : manages

    INTEGRATION ||--|| EXECUTION : enables
    INTEGRATION ||--|| MULTI_AGENT : supports

    EVALUATION ||--o{ ALL_PATTERNS : assesses

    GOAL_CREATION {
        string Passive_Goal_Creator
        string Proactive_Goal_Creator
    }

    CONTEXT_MANAGEMENT {
        string Prompt_Response_Optimiser
        string RAG
    }

    MODEL_QUERYING {
        string One_shot_Querying
        string Incremental_Querying
    }

    PLAN_GENERATION {
        string Single_path_Generator
        string Multi_path_Generator
    }

    REFLECTION {
        string Self_reflection
        string Cross_reflection
        string Human_Reflection
    }

    MULTI_AGENT {
        string Voting_based
        string Role_based
        string Debate_based
    }

    SAFETY {
        string Multimodal_Guardrails
    }

    INTEGRATION {
        string Tool_Agent_Registry
        string Agent_Adapter
    }

    EVALUATION {
        string Agent_Evaluator
    }

    EXECUTION {
        string Task_Execution
    }

    ALL_PATTERNS {
        string all_18_patterns
    }
```

## 4. íŒ¨í„´ ì„ íƒ ë§¤íŠ¸ë¦­ìŠ¤

| ìš”êµ¬ì‚¬í•­ | ì¶”ì²œ íŒ¨í„´ | ëŒ€ì•ˆ íŒ¨í„´ | ë³´ì™„ íŒ¨í„´ |
|---------|----------|----------|----------|
| **ì ‘ê·¼ì„± í–¥ìƒ** | Proactive Goal Creator | Passive Goal Creator | Multimodal Guardrails |
| **íš¨ìœ¨ì„± ìš°ì„ ** | One-shot Model Querying, Passive Goal Creator | Incremental Model Querying | Single-path Plan Generator |
| **ì¶”ë¡  í™•ì‹¤ì„±** | Incremental Model Querying, Self-reflection | One-shot Model Querying | Cross-reflection, RAG |
| **ì¸ê°„ ì„ í˜¸ ì •ë ¬** | Human Reflection, Multi-path Plan Generator | Self-reflection | Prompt/Response Optimiser |
| **í™•ì¥ì„±** | Role-based Cooperation, Cross-reflection | Voting-based Cooperation | Tool/Agent Registry |
| **ë°ì´í„° í”„ë¼ì´ë²„ì‹œ** | RAG | Fine-tuning | Multimodal Guardrails |
| **ë¹„ìš© ìµœì í™”** | One-shot Model Querying | Incremental Model Querying | Self-reflection |
| **ì„¤ëª… ê°€ëŠ¥ì„±** | Incremental Model Querying, Self-reflection | One-shot Model Querying | Human Reflection, Debate-based |
| **ì•ˆì „ì„±** | Multimodal Guardrails | Human Reflection | Self-reflection |
| **ìƒí˜¸ìš´ìš©ì„±** | Agent Adapter, Tool/Agent Registry | - | Prompt/Response Optimiser |
| **ê³µì •ì„±** | Voting-based Cooperation | Role-based Cooperation | Human Reflection |
| **ì ì‘ì„±** | Debate-based Cooperation, Agent Adapter | Role-based Cooperation | Agent Evaluator |

## 5. íŒ¨í„´ ì¡°í•© ì˜ˆì‹œ

### ì˜ˆì‹œ 1: ê³ ì‹ ë¢°ì„± ì—ì´ì „íŠ¸
```mermaid
graph LR
    A[Passive Goal Creator] --> B[Prompt/Response Optimiser]
    B --> C[RAG]
    C --> D[Incremental Model Querying]
    D --> E[Single-path Plan Generator]
    E --> F[Self-reflection]
    F --> G[Multimodal Guardrails]
    G --> H[Agent Evaluator]
```

### ì˜ˆì‹œ 2: í˜‘ì—… ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ
```mermaid
graph LR
    A[Proactive Goal Creator] --> B[Multi-path Plan Generator]
    B --> C[Cross-reflection]
    C --> D[Role-based Cooperation]
    D --> E[Tool/Agent Registry]
    E --> F[Agent Adapter]
    F --> G[Agent Evaluator]
```

### ì˜ˆì‹œ 3: ì¸ê°„ ì¤‘ì‹¬ ì—ì´ì „íŠ¸
```mermaid
graph LR
    A[Passive Goal Creator] --> B[Incremental Model Querying]
    B --> C[Multi-path Plan Generator]
    C --> D[Human Reflection]
    D --> E[Multimodal Guardrails]
    E --> F[Agent Evaluator]
```

### ì˜ˆì‹œ 4: ë¹„ìš© íš¨ìœ¨ì  ì—ì´ì „íŠ¸
```mermaid
graph LR
    A[Passive Goal Creator] --> B[One-shot Model Querying]
    B --> C[Single-path Plan Generator]
    C --> D[Self-reflection]
    D --> E[Agent Evaluator]
```

## 6. íŒ¨í„´ ì„ íƒ ì²´í¬ë¦¬ìŠ¤íŠ¸

### Step 1: ëª©í‘œ ìƒì„± ë°©ì‹ ê²°ì •
- [ ] ì‚¬ìš©ìê°€ ëª…í™•í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ì œê³µí•˜ëŠ”ê°€? â†’ Passive Goal Creator
- [ ] ë©€í‹°ëª¨ë‹¬ ì»¨í…ìŠ¤íŠ¸ ìº¡ì²˜ê°€ í•„ìš”í•œê°€? â†’ Proactive Goal Creator
- [ ] ì ‘ê·¼ì„±ì´ ì¤‘ìš”í•œê°€? â†’ Proactive Goal Creator

### Step 2: ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬ ê²°ì •
- [ ] í”„ë¡¬í”„íŠ¸ í‘œì¤€í™”ê°€ í•„ìš”í•œê°€? â†’ Prompt/Response Optimiser
- [ ] ì™¸ë¶€ ì§€ì‹ ë² ì´ìŠ¤ê°€ í•„ìš”í•œê°€? â†’ RAG
- [ ] ë°ì´í„° í”„ë¼ì´ë²„ì‹œê°€ ì¤‘ìš”í•œê°€? â†’ RAG

### Step 3: ëª¨ë¸ ì¿¼ë¦¬ ì „ëµ ê²°ì •
- [ ] ë¹„ìš©ì´ ì œí•œì ì¸ê°€? â†’ One-shot Model Querying
- [ ] ìƒì„¸í•œ ì¶”ë¡  ê³¼ì •ì´ í•„ìš”í•œê°€? â†’ Incremental Model Querying
- [ ] ì„¤ëª… ê°€ëŠ¥ì„±ì´ ì¤‘ìš”í•œê°€? â†’ Incremental Model Querying

### Step 4: ê³„íš ìƒì„± ë°©ì‹ ê²°ì •
- [ ] íš¨ìœ¨ì„±ì´ ìµœìš°ì„ ì¸ê°€? â†’ Single-path Plan Generator
- [ ] ì‚¬ìš©ì ë§ì¶¤í™”ê°€ í•„ìš”í•œê°€? â†’ Multi-path Plan Generator
- [ ] ë³µì¡í•œ ì‘ì—…ì¸ê°€? â†’ Multi-path Plan Generator

### Step 5: ê²€í†  ë©”ì»¤ë‹ˆì¦˜ ê²°ì •
- [ ] ìë™í™”ëœ ê²€í† ê°€ í•„ìš”í•œê°€? â†’ Self-reflection
- [ ] ë‹¤ì–‘í•œ ê´€ì ì´ í•„ìš”í•œê°€? â†’ Cross-reflection
- [ ] ì¸ê°„ì˜ íŒë‹¨ì´ ì¤‘ìš”í•œê°€? â†’ Human Reflection

### Step 6: í˜‘ë ¥ ë°©ì‹ ê²°ì • (ë‹¤ì¤‘ ì—ì´ì „íŠ¸ì˜ ê²½ìš°)
- [ ] ê³µì •í•œ ì˜ì‚¬ê²°ì •ì´ í•„ìš”í•œê°€? â†’ Voting-based
- [ ] ì „ë¬¸í™”ëœ ì—­í•  ë¶„ë‹´ì´ í•„ìš”í•œê°€? â†’ Role-based
- [ ] ì ì‘ì  í•™ìŠµì´ í•„ìš”í•œê°€? â†’ Debate-based

### Step 7: ì•ˆì „ì„± ë° í†µí•© ê²°ì •
- [ ] ì…ì¶œë ¥ ì œì–´ê°€ í•„ìš”í•œê°€? â†’ Multimodal Guardrails
- [ ] ì™¸ë¶€ ë„êµ¬/ì—ì´ì „íŠ¸ ê´€ë¦¬ê°€ í•„ìš”í•œê°€? â†’ Tool/Agent Registry
- [ ] ë„êµ¬ ì¸í„°í˜ì´ìŠ¤ ë³€í™˜ì´ í•„ìš”í•œê°€? â†’ Agent Adapter

### Step 8: í‰ê°€ ê²°ì •
- [ ] ì„±ëŠ¥ í‰ê°€ê°€ í•„ìš”í•œê°€? â†’ Agent Evaluator

## 7. íŒ¨í„´ ì ìš© ìš°ì„ ìˆœìœ„

```mermaid
graph TD
    subgraph Priority_1[ìš°ì„ ìˆœìœ„ 1: í•µì‹¬ ê¸°ëŠ¥]
        Core1[Goal Creator<br/>Passive/Proactive]
        Core2[Plan Generator<br/>Single/Multi-path]
    end

    subgraph Priority_2[ìš°ì„ ìˆœìœ„ 2: í’ˆì§ˆ í–¥ìƒ]
        Quality1[Reflection<br/>Self/Cross/Human]
        Quality2[Model Querying<br/>One-shot/Incremental]
    end

    subgraph Priority_3[ìš°ì„ ìˆœìœ„ 3: ìµœì í™”]
        Opt1[Prompt/Response Optimiser]
        Opt2[RAG]
    end

    subgraph Priority_4[ìš°ì„ ìˆœìœ„ 4: ì•ˆì „ì„±]
        Safety1[Multimodal Guardrails]
    end

    subgraph Priority_5[ìš°ì„ ìˆœìœ„ 5: í™•ì¥]
        Ext1[Multi-Agent Cooperation]
        Ext2[Tool/Agent Registry]
        Ext3[Agent Adapter]
    end

    subgraph Priority_6[ìš°ì„ ìˆœìœ„ 6: ê²€ì¦]
        Val1[Agent Evaluator]
    end

    Priority_1 --> Priority_2
    Priority_2 --> Priority_3
    Priority_3 --> Priority_4
    Priority_4 --> Priority_5
    Priority_5 --> Priority_6
```

## ë²”ë¡€

**ê´€ê³„ íƒ€ì…:**
- ì‹¤ì„  í™”ì‚´í‘œ (â†’): ì§ì ‘ì ì¸ ì—°ê²°/ì˜ì¡´ì„±
- ì ì„  í™”ì‚´í‘œ (-.->): ëŒ€ì•ˆ ê´€ê³„
- ì´ì¤‘ì„  (==>): ê°•í•œ ì˜ì¡´ì„±
- "ë³´ì™„": í•¨ê»˜ ì‚¬ìš©í•˜ë©´ ì‹œë„ˆì§€

**ìƒ‰ìƒ ì˜ë¯¸:**
- ğŸŸ¢ ë…¹ìƒ‰: ëª©í‘œ ìƒì„± ê´€ë ¨
- ğŸŸ¡ ë…¸ë€ìƒ‰: ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬
- ğŸ”µ íŒŒë€ìƒ‰: ë°ì´í„°/ì§€ì‹ ê´€ë ¨
- ğŸ”´ ë¶„í™/ë¹¨ê°•: ì¶”ë¡  ë° ê²€í† 
- ğŸŸ£ ë³´ë¼: ê³„íš ìƒì„±
- ğŸŸ  ì£¼í™©: ì•ˆì „ì„±
- ğŸ”· ì²­ë¡: í†µí•© ë° ë„êµ¬
